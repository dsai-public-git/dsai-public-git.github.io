<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title><![CDATA[DSAI | Data Science and AI Association of Australia]]></title>
    <link href="/blog/tags/ai.xml" rel="self"/>
    <link href="/blog/"/>
    <updated>2020-10-13T05:33:54+00:00</updated>
    <id>/blog/</id>
        <generator uri="http://sculpin.io/">Sculpin</generator>
            <entry>
            <title type="html"><![CDATA[Unprecedented times – leading new conversations with data]]></title>
            <link href="/blog/2020/09/10/unprecedented-times-leading-new-conversations-with-data"/>
            <updated>2020-09-10T00:00:00+00:00</updated>
            <id>/blog/2020/09/10/unprecedented-times-leading-new-conversations-with-data</id>
            <content type="html"><![CDATA[<p>The term "unprecedented" has been repeated often in 2020. It’s a word
that has come to symbolize the chaos and disruption that has left many
people and companies in turmoil. Despite this, "unprecedented" does not
have to just have negative connotations though. There is the opportunity
to create unprecedented good in this world too. With <strong>Arturo
Rodriguez</strong> and I coming onboard to the DSAi community I think there is
an unprecedented opportunity and that is to have more amazing
conversations around the power of data and the technologies out there
than can turn raw information into powerful insights.</p>

<p> </p>

<p>For those who don't know, DSAi was built from a collective of separate
data science communities and brought together thanks to
co-founders <strong>Jacky Koh, Paul Steven Conyngham </strong>and<strong> Shuning Zhao</strong>.
Over their short period of existence, they've grown from strength to
strength and we're going to continue developing the value that a
community like this can bring.</p>

<p> </p>

<p>2020 has been a crazy year for people in many industries but data and
technology can be a catalyst to change and improve the world and give
people a chance to control the narrative. Whether you're a decision
maker, manager, analyst or someone just starting to get invovled in
data, then come check out DSAi and register your interest in the link
below so you too can become part of an amazing community.</p>

<p>We’re going to be focusing on 3 types of events that the data community
in Australia (and globally) can be a part and we’re looking forward to
those who can participate as well as those who have ideas for how they
can contribute to the conversation here.</p>

<p> </p>

<p>The 3 event types are made to help facilitate conversations for
different levels of users. These range from <strong>Getting Started </strong>events
for the novice data analysts and those interested in getting started
to <strong>Masterclasses </strong>for the more senior and advanced people working
with data. Finally, we will also be catering towards the decision makers
on the data community and helping them understand the latest and
greatest goings on in the the world of data. Bringing all of these sorts
of people together in one place means we’ll all be talking the same
language and we hope will lead to more meaningful conversations about
data. When it comes to talking about data we want DSAi to be the place
to go to and we want to make sure everyone gets value from it.</p>

<p> </p>

<h2 id="%2A%2Aevent-types%2A%2A"><strong>Event types</strong></h2>

<ul>
<li><strong>Getting Started</strong> – these events are for those getting started and
we will have speakers present examples of work that they have done.
This ranges from crunching the numbers with data science tools like
Python and R to visualization techniques in data analytics
applications like Qlik, Tableau or Power BI.</li>
<li><strong>Masterclasses </strong>– these events are geared towards more experienced
people across different industries to talk about specific problems
they’ve solved as well as coming together to help each other solve
problems. There may be some data war stories or examples of how to
get projects done better.</li>
<li><strong>Interviews and Roundtables </strong>– this type of event brings in more
of the decision makers and business end of the scale into the data
conversation. Whilst a data community can get a lot done by
collaborating with one another, we want to get more parts of the
business into the conversation too so we can better understand their
pain points and help each other with better ways of working. These
interviews (and eventually roundtables) will help us understand
that.</li>
</ul>

<p>It's going to be an exciting period going forward for DSAi and we look
forward to being part of your data journey!</p>
]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Transformers: more than meets the eye? (Part 1: The Past)]]></title>
            <link href="/blog/2019/06/18/transformers-more-than-meets-the-eye-part-i-the-past"/>
            <updated>2019-06-18T00:00:00+00:00</updated>
            <id>/blog/2019/06/18/transformers-more-than-meets-the-eye-part-i-the-past</id>
            <content type="html"><![CDATA[<pre><code>Original article and Stephen's blog can be found here: https://breakitdownto.earth/2019/05/10/Transformers_part1.html

Reposted with permission.
</code></pre>

<p>There’s quite a lot of talk about how wonderful Transformers are and how they solve a lot of problems. Fair enough, they are pretty good. That said, all the explanations I’ve seen so far include math and neural network diagrams (nothing wrong with that!). What I’m going to attempt to do is talk about how they work in a more conceptual way, because it’s much easier to understand the math and diagrams if you have some idea of why those equations even exist.</p>

<p>In this post I’ll attempt to walk through the history that got us to the point where Transformers (the neural network technique, for clarity), seemed like a good idea. There are a couple of things we’ll need to think about:</p>

<ul>
<li>The situation we use a Transformer in - and why</li>
<li>How have we evolved in thinking about how to deal with natural language?</li>
<li>Why wasn’t any of the stuff we thought of in the past good enough? Why did we have to keep going and invent transformers?</li>
<li>How do transformers work?</li>
</ul>

<p>We’ll look at the first two points in this post and the second two in the next post. I’ll be assuming you know what you’re doing with neural networks, to at least some extent.</p>

<p>Hopefully what you’ll see at the end of all of this is that Transformers contain echoes of all the things that came before and that in conceptual terms they’re actually quite simple. Finally, I’m going to talk about this through the lens of Natural Language Processing (NLP), because that’s what these things get used for. If you’re interested in this with respect to financial time series stuff or otherwise, just squint really hard and imagine some of the words are replaced with whatever the appropriate domain-specific replacement is.</p>

<h2 id="what-problem-is-a-transformer-meant-to-solve%3F">What problem is a transformer meant to solve?</h2>

<p>This is actually the simple bit of these posts, if anything is. It’s what is known as a <em>sequence to sequence</em> problem. Also known as a <em>seq2seq</em> problem if you’re one of the people who works on this sort of thing.</p>

<p>Now, what the heck is that?</p>

<p>Basically, it’s the sort of thing you see over and over in NLP. Translation is a perfect example. Take for instance the following input and output pair:</p>

<blockquote>
  <p>Input : I do want to eat vegetables Output: 我想吃蔬菜</p>
</blockquote>

<p>The goal of this task is to turn the input (English), into the output (Mandarin). Clearly, the input is just a bunch of words and so is the output. BUT! The order in which they appear matters. To see this in action, the input “do I want to eat vegetables” clearly has a different meaning. Some languages might have less of a problem with this than others, but in any event it’s a real source of information that we want to incorporate when we’re working on this problem. So the challenge is, how to predict a sequence of values - not necessarily words, it could be a financial time series problem you’re working on - using not just the knowledge of the input values but their structure (i.e. order), as well.</p>

<p>Just to briefly emphasise, this general type of problem has existed forever. People creating neural networks didn’t invent this type of problem, only new ways of solving it.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref">1</a></sup></p>

<h2 id="important-digression%21">Important digression!</h2>

<p>Looking at the above example and a great deal of explanatory material on the topic of <em>seq2seq</em> problems, there’s a common problem you can encounter. It’s this: the problem looks like the input and output are concurrent, but they actually aren’t.</p>

<p>The way the problem works is, you feed in an input sequence and then want your model to complete the sequence. So really, the input and output pair above should be presented more like this<sup id="fnref:4"><a href="#fn:4" class="footnote-ref">2</a></sup>:</p>

<blockquote>
  <p>I do want to eat vegetables <strong>BREAK</strong> 我想吃蔬菜</p>
</blockquote>

<p>The reason for the <em>BREAK</em> there is that you need something to tell your model to stop reading input and start generating output.</p>

<p>This will become very important as time goes on in this discussion.</p>

<h2 id="baby-steps">Baby steps</h2>

<p>Ok so first let’s step back in time.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref">3</a></sup> In the beginning (and I mean <em>actually</em> a long time ago, like 40 years or more), we didn’t have any of the gleaming mathematical machinery we now do for working on language problems. So where to start?</p>

<h3 id="n-grams">N-grams</h3>

<p>This was the first, and simplest go at trying to figure out the <em>what comes next</em>? problem beyond just feeding in your input and hoping for the best. The idea here is that you can actually do a better job of predicting the next word if you use not only the current one, but the one before it. All the “N” in “N-grams” tells you is that you might use more or less context as opposed to specifically one or two words of history. The following examples might make things clearer.</p>

<p>Guess the next word given the input:</p>

<blockquote>
  <p>is</p>
</blockquote>

<p>This is basically impossible, right? There’s some stuff that might not be that likely to come next, but by and large we can’t really narrow it down.</p>

<p>What about this input:</p>

<blockquote>
  <p>weather is</p>
</blockquote>

<p>This is a bit more workable. We could probably make a few guesses about what goes next, but it’s definitely a narrower scope of possibilities than last time.</p>

<p>Now what about this:</p>

<blockquote>
  <p>The weather is</p>
</blockquote>

<p>At this point I’m fairly sure something like <em>hot</em> or <em>cold</em> comes next.</p>

<p>So far so good, but what’s the cost? It’s pretty simple, in fact. Firstly, the reason we know one of those words comes next is because we’ve all watched too much news on the TV so far in this life. That’s well and good for a human, but sometimes you don’t have enough data for that to work for a machine.</p>

<p>Another problem arises if you’re trying to predict more than just the next word. If you’re trying to predict a whole sentence, you’re in a lot of trouble. Maybe you can get the first word of your output correct, but after that you’re at the mercy of whatever you predicted as your first word. If that’s wrong, everything else becomes even wronger. We’ve only really started solving this problem comparatively recently.</p>

<p>Even worse, the longer your input sequence the rarer it is! In the case of “The weather is”, you might be able to see it a few times in a dataset if you’re lucky. But not as many times as you see “weather is”, perhaps. So we’re stuck in a classic problem where we can get a few edge cases right all of the time, or have hopefully better than random performance over a much broader set of cases, assuming your input doesn’t fundamentally change as time goes on. That’s probably not good enough. So people kept researching stuff.</p>

<h2 id="time-becomes-a-loop%5E3">Time becomes a loop<sup id="fnref:3"><a href="#fn:3" class="footnote-ref">4</a></sup></h2>

<p>Another thing that people discovered when talking about <em>seq2seq</em> problems is that sometimes stuff a long time ago in your input sequence actually matters in terms of the prediction you make at the end of it. Doing stuff with N-grams won’t work here because for example a 128-gram is basically unique. You’ll only every see that sequence of tokens the once, if at all, so it’s no use for prediction. So what can you do about this?</p>

<p>This is where things start getting complicated, unfortunately. That is to say, as soon as you try and do anything beyond the most basic stuff with a <em>seq2seq</em> problem, it gets complicated.</p>

<p>Intuitively, what we want is something that can remember the earlier parts of a sequence like this:4</p>

<blockquote>
  <p>I was born in a water moon. Some people, especially its inhabitants, called it a planet, but as it was only a little over two hundred kilometres in diameter, ‘moon’ seems the more accurate term. The moon was made entirely of water, by which I mean it was a globe that not only had no land, but no rock either, a sphere with no solid core at all, just liquid water, all the way down to the very centre of the globe.<br />
      If it had been much bigger the moon would have had a core of ice, for water, though supposedly incompressible, is not entirely so, and will change under extremes of pressure to become ice. (If you are used to living on a planet where ice floats on the surface of water, this seems odd and even wrong, but nevertheless it is the case.) The moon was not quite of a size for an ice core to form, and therefore one could, if one was sufficiently hardy, and adequately proof against the water pressure, make one’s way down, through the increasing weight of water above, to the very centre of the moon.<br />
      Where a strange thing happened.<br />
      For here, at the very centre of this watery globe, there seemed to be no gravity. There was colossal pressure, certainly, pressing in from every side, but one was in effect weightless (on the outside of a planet, moon or other body, watery or not, one is always being pulled towards its centre; once at its centre one is being pulled equally in all directions), and indeed the pressure around one was, for the same reason, not quite as great as one might have expected it to be, given the mass of water that the moon was made up from.<br />
      This was, of course,</p>
</blockquote>

<p>Imagine you’re trying to guess what comes next after “This was, of course,”. Do you think it’s something about the physics stuff towards the end of the passage, or about the perspective of people who live on planets where ice floats on the surface referred to much earlier in the text? It could be either, but if your model isn’t capable of having some way of hanging onto relevant info from much earlier in your input sequence, you have no choice. You can only really consider the physics option.</p>

<p>So we needed some way of hanging onto not everything but at least some important info from the earlier parts of a potentially long sequence. This is where neural networks start to play a role.</p>

<p>The original go at this involved trying to get a neural network to have a memory of its own, after a fashion.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref">5</a></sup> I don’t want to go on forever about this because it turns into talking about math, but the long and the short of it was that you wanted a bit of your network to loop around on itself in a way that made some of the information coming in from a given sequence element take a while to get back out again.</p>

<p>It’s a bit like Gumbo. <sup id="fnref:6"><a href="#fn:6" class="footnote-ref">6</a></sup> This is a slightly weird analogy, but stick with me. Gumbo is this dish you get in the southern parts of the U.S. that takes <em>forever</em> to cook. But the trick is to make it so that the flavour of the ingredients you put in early on is still retained some hours later in the pot when you finally finish cooking, even if you’ve taken that ingredient back out.</p>

<p>In our case, we want to put words in and move on to subsequent parts of the sequence without necessarily losing all of the earlier flavour. By the time we get to the end of that passage I put in above, we still want to be able to have some hint of the earlier part where the speaker says his planet was a moon.</p>

<p>In practical terms, what you do is make information take a really long path to get out of your network. Problem is, this isn’t perfect either. For a really long sequence, you can wind up with a gigantic number of parameters in your network. You also don’t really know what to hang on to, so by creating a recurrent neural network all you’re really doing is trying to hang on to as much of your input information as possible for as long as possible. This is on top of the other problem where introducing recurrence makes it slower to process an input sequence because now you have a dependency problem: you can only process some of your input after you’ve processed the bits that came before.</p>

<p>That said, it’s still a big big step forward.</p>

<h2 id="my-model-accurately-predicts-everything%21">My model accurately predicts everything!</h2>

<p>The next step forward after recurrence seemed at first like it solved everything. We’ll see that wasn’t the case though.</p>

<p>It turns out two of the problems with the basic approach to recurrence can be solved in the same way. The non-selectiveness of memory storage in a basic recurrent net and the escalating number of parameters needed were both addressed by the Long Short-Term Memory cell.</p>

<p>What this thing did differently was conceptually simple enough. Rather than just adding links between cells in a neural network so that it took a while for information to find the exit, each cell now got its own memory storage.</p>

<p>As always, so far so good. With this particular invention, you could get to the end of a passage like the one in the previous section and have some sort of useful numerical information from earlier on predict what comes next.</p>

<p>Going back to the first example I gave:</p>

<blockquote>
  <p>I do want to eat vegetables <strong>BREAK</strong> 我想吃蔬菜</p>
</blockquote>

<p>Now that we have an LSTM to work with, the input sequence can comfortably be quite large and we’ll still have all of it to work with when we start predicting an output sequence.</p>

<p>That’s another big step forward, but there’s still a problem here. Now, although we have a lot of information to work with when we start predicting an output sequence, we still have the problem that the sequence is only as good as the first prediction.</p>

<p>That is to say, we need the entire sequence to be meaningfully encoded in whatever you have after you’ve looked at the last item in your input. It also has to be able to let you spell out the entire output sequence. And when you do, you’re still stuck with the problem that your second prediction depends on your first one and your third prediction depends on your second one.</p>

<p>Also this LSTM stuff is relatively slow because you have to process one element of your sequence at a time and there’s no real way around that in the framework. That said, as always, it was a big leap forwards and started to show some serious potential for dealing with <em>seq2seq</em> problems.</p>

<p>Wouldn’t it make more sense and potentially be faster if we could use all of the bits of the input sequence independently in determining an output, instead of pancaking the information you have up against an invisible wall separating your input sequence and whatever comes next? This question is more or less what the Attention method and Transformers grow out of. I’ll talk about why next time.</p>

<div class="footnotes">
<hr />
<ol>

<li id="fn:1">
<p>Just in case anyone was wondering.&#160;<a href="#fnref:1" class="footnote-backref">&#8617;</a></p>
</li>

<li id="fn:4">
<p>From <em>The Algebraist</em> by Iain M. Banks. R.I.P.&#160;<a href="#fnref:4" class="footnote-backref">&#8617;</a></p>
</li>

<li id="fn:2">
<p>This is secretly a joke. If you work out what I’m referring to, I’m not sorry.&#160;<a href="#fnref:2" class="footnote-backref">&#8617;</a></p>
</li>

<li id="fn:3">
<p>https://www.youtube.com/watch?v=DNb4VKln1uw&#160;<a href="#fnref:3" class="footnote-backref">&#8617;</a></p>
</li>

<li id="fn:5">
<p>Not as cool as anything invented by Miles Bennett Dyson, but still.&#160;<a href="#fnref:5" class="footnote-backref">&#8617;</a></p>
</li>

<li id="fn:6">
<p>https://en.wikipedia.org/wiki/Gumbo#Preparation_and_serving&#160;<a href="#fnref:6" class="footnote-backref">&#8617;</a></p>
</li>

</ol>
</div>
]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Machine Learning Jokes]]></title>
            <link href="/blog/2019/05/20/machine-learning-jokes"/>
            <updated>2019-05-20T00:00:00+00:00</updated>
            <id>/blog/2019/05/20/machine-learning-jokes</id>
            <content type="html"><![CDATA[<p><strong>Machine Learning Jokes</strong></p>

<p><img src="/blog/images/jokes.jpg" alt="Robot performing a stand up comedy routine."></p>

<p>If you don’t find this funny, well, its time to brush up your linear algebra and multivariable calculus.</p>

<ol>
<li>Why are neural networks better than humans ? Because they actually learn from their mistakes.</li>
<li>Why is machine learning so racist ? Because it divides everything in classes. </li>
<li>What do you call glorified curve fitting ? Deep learning.</li>
<li>Why can’t random forest learn anything on its own ? Because it needs supervision.</li>
<li>Data scientist goes in for a kidney transplant. Asks the surgeon “ Have you done this before “ The surgeon says “ No, but I have watched the first 10 minutes of ‘Learn surgery in 1 hour’". </li>
<li>How do you raise a billion dollar for your coffee business in silicon valley ? You rename the business to Deep.Coffee.AI </li>
<li>What is Ian Goodfellow's favourite catch phrase ? Yes we GAN. </li>
<li>1 Million robots marched towards the cliff, 999999 fell off the cliff and were destroyed but 1 didn't, why ? Because it takes a robot that many examples to learn something. </li>
<li>Whats the difference between a LSTM and Donald Trump ? A LSTM is capable of generating perfect English sentences. </li>
<li>What did the cost function say to the waiter after looking at the dinner bill ? Hey can you minimize that. </li>
</ol>
]]></content>
        </entry>
    </feed>